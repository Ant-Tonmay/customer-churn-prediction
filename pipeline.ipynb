{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ca0d36-0ae0-49e0-b84d-a5e54a6e6bc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T09:39:03.933629Z",
     "iopub.status.busy": "2025-09-22T09:39:03.933324Z",
     "iopub.status.idle": "2025-09-22T09:39:06.722722Z",
     "shell.execute_reply": "2025-09-22T09:39:06.721950Z",
     "shell.execute_reply.started": "2025-09-22T09:39:03.933603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.processing import FrameworkProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker.estimator import InstanceGroup\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThan\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "from sagemaker import ModelPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f375f2-a91b-4ad4-8621-b27552e2d4fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T09:42:56.860209Z",
     "iopub.status.busy": "2025-09-22T09:42:56.859927Z",
     "iopub.status.idle": "2025-09-22T09:42:56.880050Z",
     "shell.execute_reply": "2025-09-22T09:42:56.879066Z",
     "shell.execute_reply.started": "2025-09-22T09:42:56.860185Z"
    }
   },
   "outputs": [],
   "source": [
    "class ChurnPredictionPipeline:\n",
    "    def __init__(self, raw_data_s3_uri, output_bucket, output_prefix,\n",
    "                 artifacts_path):\n",
    "        self.execution = None\n",
    "        self.sagemaker_session = sagemaker.Session()\n",
    "        self.role = sagemaker.get_execution_role()\n",
    "        self.bucket = self.sagemaker_session.default_bucket()\n",
    "        self.region = self.sagemaker_session.boto_region_name\n",
    "        self.raw_data_s3_uri = raw_data_s3_uri\n",
    "        self.output_bucket = output_bucket\n",
    "        self.output_prefix = output_prefix\n",
    "        self.artifacts_path = artifacts_path\n",
    "        self.pipeline_session = PipelineSession()\n",
    "        self.cache_config = CacheConfig(enable_caching=True, expire_after=\"30d\")\n",
    "\n",
    "    def create_data_prep_stage(self):\n",
    "        sklearn_processor = SKLearnProcessor(\n",
    "            framework_version=\"1.2-1\",\n",
    "            role=self.role,\n",
    "            instance_type=\"ml.t3.medium\",\n",
    "            instance_count=1,\n",
    "        )\n",
    "        data_prep = ProcessingStep(\n",
    "            name=\"ChurnDataPrep\",\n",
    "            processor=sklearn_processor,\n",
    "            code=\"scripts/data_ingestion.py\",\n",
    "            inputs=[\n",
    "                ProcessingInput(source=self.raw_data_s3_uri, destination=\"/opt/ml/processing/input\")\n",
    "            ],\n",
    "            outputs=[\n",
    "                ProcessingOutput(\n",
    "                    output_name=\"train\",\n",
    "                    source=\"/opt/ml/processing/output/train\",\n",
    "                    destination=\"s3://sagemakerantdata/smallchurndataset/processed/train\"\n",
    "                ),\n",
    "                ProcessingOutput(\n",
    "                    output_name=\"test\",\n",
    "                    source=\"/opt/ml/processing/output/test\",\n",
    "                    destination=\"s3://sagemakerantdata/smallchurndataset/processed/test\"\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "        return data_prep\n",
    "\n",
    "    def model_traing_and_tuning(self, data_prep):\n",
    "        output_bucket = self.output_bucket\n",
    "        output_prefix = self.output_prefix\n",
    "        output_path = f\"s3://{output_bucket}/{output_prefix}\"\n",
    "\n",
    "        sklearn_estimator_rf = SKLearn(\n",
    "                entry_point=\"scripts/train.py\",\n",
    "                framework_version=\"1.2-1\",\n",
    "                instance_type=\"ml.m5.large\",\n",
    "                instance_count=1,\n",
    "                role=self.role,\n",
    "                hyperparameters={\"model_type\": \"randomforest\"},\n",
    "                output_path=output_path\n",
    "            )\n",
    "        tuner_rf = HyperparameterTuner(\n",
    "            estimator=sklearn_estimator_rf,\n",
    "            objective_metric_name=\"Validation F1 Score\",\n",
    "            hyperparameter_ranges={\n",
    "                \"n_estimators\": IntegerParameter(100, 300),\n",
    "                \"max_depth\": IntegerParameter(5, 20),\n",
    "            },\n",
    "            metric_definitions=[\n",
    "                {\"Name\": \"Validation F1 Score\", \"Regex\": \"Validation F1 Score: ([0-9\\\\.]+)\"}\n",
    "            ],\n",
    "            max_jobs=6,\n",
    "            max_parallel_jobs=2,\n",
    "            objective_type=\"Maximize\",\n",
    "        )\n",
    "        tune_step_rf = TuningStep(\n",
    "            name=\"TuneRandomForest\",\n",
    "            tuner=tuner_rf,\n",
    "            inputs={\n",
    "                \"train\": TrainingInput(\n",
    "                    s3_data=data_prep.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri\n",
    "                ),\n",
    "                \"test\": TrainingInput(\n",
    "                    s3_data=data_prep.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri\n",
    "                ),\n",
    "            },\n",
    "            cache_config=self.cache_config\n",
    "        )\n",
    "        # Logistic Regression\n",
    "        logistic_estimator = SKLearn(\n",
    "            entry_point=\"scripts/train.py\",\n",
    "            framework_version=\"1.2-1\",\n",
    "            instance_type=\"ml.m5.large\",\n",
    "            instance_count=1,\n",
    "            role=self.role,\n",
    "            hyperparameters={\"model_type\": \"logistic\"},\n",
    "            output_path=output_path\n",
    "        )\n",
    "        tuner_logistic = HyperparameterTuner(\n",
    "            estimator=logistic_estimator,\n",
    "            objective_metric_name=\"Validation F1 Score\",\n",
    "            hyperparameter_ranges={\n",
    "                \"C\": ContinuousParameter(0.001, 10.0),\n",
    "                # penalty can only take [\"l1\", \"l2\"], so not tunable like numeric params\n",
    "            },\n",
    "            metric_definitions=[\n",
    "                {\"Name\": \"Validation F1 Score\", \"Regex\": \"Validation F1 Score: ([0-9\\\\.]+)\"}\n",
    "            ],\n",
    "            max_jobs=4,\n",
    "            max_parallel_jobs=2,\n",
    "            objective_type=\"Maximize\",\n",
    "        )\n",
    "        tune_step_logistic = TuningStep(\n",
    "            name=\"TuneLogisticRegression\",\n",
    "            tuner=tuner_logistic,\n",
    "            inputs={\n",
    "                \"train\": TrainingInput(\n",
    "                    s3_data=data_prep.\n",
    "                    properties.\n",
    "                    ProcessingOutputConfig.\n",
    "                    Outputs[\"train\"].S3Output.S3Uri\n",
    "                ),\n",
    "                \"test\": TrainingInput(\n",
    "                    s3_data=data_prep.\n",
    "                    properties.\n",
    "                    ProcessingOutputConfig.\n",
    "                    Outputs[\"test\"].S3Output.S3Uri\n",
    "                ),\n",
    "            },\n",
    "            cache_config=self.cache_config\n",
    "        )\n",
    "        return [tune_step_rf, tune_step_logistic]\n",
    "\n",
    "    def register_best_model(self, tune_step_rf, tune_step_logistic):\n",
    "        best_rf_f1 = (\n",
    "                tune_step_rf\n",
    "                .properties\n",
    "                .BestTrainingJob\n",
    "                .FinalHyperParameterTuningJobObjectiveMetric\n",
    "                .Value\n",
    "        )\n",
    "        best_logistic_f1 = (\n",
    "                tune_step_logistic\n",
    "                .properties\n",
    "                .BestTrainingJob\n",
    "                .FinalHyperParameterTuningJobObjectiveMetric\n",
    "                .Value\n",
    "        )\n",
    "        rf_model = SKLearnModel(\n",
    "            model_data=tune_step_rf.get_top_model_s3_uri(\n",
    "                             top_k=1,\n",
    "                             s3_bucket=self.artifacts_path\n",
    "                        ),\n",
    "            role=self.role,\n",
    "            entry_point=\"scripts/inference.py\",\n",
    "            framework_version=\"1.2-1\",\n",
    "            sagemaker_session=self.pipeline_session,\n",
    "        )\n",
    "        rf_register_args = rf_model.register(\n",
    "            content_types=[\"text/csv\"],\n",
    "            response_types=[\"text/csv\"],\n",
    "            inference_instances=[\"ml.m5.large\"],\n",
    "            transform_instances=[\"ml.m5.large\"],\n",
    "            model_package_group_name=\"ChurnPredictionModelGroup\",\n",
    "        )\n",
    "        step_register_rf_model = ModelStep(\n",
    "            name=\"RegisterRFModelConditional\",\n",
    "            step_args=rf_register_args,\n",
    "        )\n",
    "        # Same for logistic:\n",
    "        log_model = SKLearnModel(\n",
    "            model_data=tune_step_logistic.get_top_model_s3_uri(\n",
    "                                                top_k=1,\n",
    "                                                s3_bucket=self.artifacts_path\n",
    "                                            ),\n",
    "            role=self.role,\n",
    "            entry_point=\"scripts/inference.py\",\n",
    "            framework_version=\"1.2-1\",\n",
    "            sagemaker_session=self.pipeline_session,\n",
    "        )\n",
    "        log_register_args = log_model.register(\n",
    "            content_types=[\"text/csv\"],\n",
    "            response_types=[\"text/csv\"],\n",
    "            inference_instances=[\"ml.m5.large\"],\n",
    "            transform_instances=[\"ml.m5.large\"],\n",
    "            model_package_group_name=\"ChurnPredictionModelGroup\",\n",
    "        )\n",
    "        step_register_logistic_model = ModelStep(\n",
    "            name=\"RegisterLogisticModelConditional\",\n",
    "            step_args=log_register_args,\n",
    "        )\n",
    "        # Conditional step to choose and register the best model\n",
    "        choose_best_model_step = ConditionStep(\n",
    "            name=\"ChooseAndRegisterBestModelStep\",  # Made name more explicit\n",
    "            conditions=[ConditionGreaterThan(left=best_rf_f1,\n",
    "                                             right=best_logistic_f1)],\n",
    "            if_steps=[step_register_rf_model],\n",
    "            else_steps=[step_register_logistic_model],\n",
    "        )\n",
    "        return choose_best_model_step\n",
    "\n",
    "    def create_pipeline(self):\n",
    "\n",
    "        data_prep = self.create_data_prep_stage()\n",
    "        [tune_step_rf, tune_step_logistic] = self. model_traing_and_tuning(\n",
    "                                            data_prep)\n",
    "        choose_best_model_step = self.register_best_model(tune_step_rf,\n",
    "                                                          tune_step_logistic)\n",
    "        pipeline = Pipeline(\n",
    "            name=\"ChurnPredictionPipelineV2\",\n",
    "            steps=[\n",
    "                data_prep,           # Data preparation step\n",
    "                tune_step_rf,        # RF hyperparameter tuning\n",
    "                tune_step_logistic,  # Logistic hyperparameter tuning\n",
    "                choose_best_model_step  # Conditional step (contains the RegisterModel steps)\n",
    "            ],\n",
    "            sagemaker_session=self.sagemaker_session,\n",
    "        )\n",
    "        return pipeline\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        pipeline = self.create_pipeline()\n",
    "        pipeline.upsert(role_arn=self.role)\n",
    "        self.execution = pipeline.start()\n",
    "        self.execution.describe()\n",
    "\n",
    "    def check_pipeline_status(self):\n",
    "        if self.execution is None:\n",
    "            raise RuntimeError(\"Pipeline has not been started yet\")\n",
    "        steps = self.execution.list_steps()\n",
    "        for step in steps:\n",
    "            print(f\"{step['StepName']} → {step['StepStatus']}\")\n",
    "            if 'FailureReason' in step:\n",
    "                print(f\"   Reason: {step['FailureReason']}\")\n",
    "\n",
    "    def get_model_arn(self, execution):\n",
    "        steps = self.execution.list_steps()\n",
    "        for step in steps:\n",
    "            if step[\"StepName\"] == \"ChooseAndRegisterBestModelStep\":\n",
    "                outcome = step[\"Metadata\"][\"Condition\"][\"Outcome\"]\n",
    "                print(\"Condition outcome:\", outcome)\n",
    "        if outcome == \"True\":  # RF was better\n",
    "            best_model_step = [s for s in steps if \"RegisterRFModelConditional\" in s[\"StepName\"]][0]\n",
    "        else:  # Logistic was better\n",
    "            best_model_step = [s for s in steps if \"RegisterLogisticModelConditional\" in s[\"StepName\"]][0]\n",
    "\n",
    "        model_arn = best_model_step[\"Metadata\"][\"RegisterModel\"][\"Arn\"]\n",
    "        print(\"Best model ARN:\", model_arn)\n",
    "        return model_arn\n",
    "\n",
    "    def sagemaker_endpoint(self):\n",
    "        model_package = get_model_arn()\n",
    "        model = ModelPackage(\n",
    "            role=role,\n",
    "            model_package_arn=model_package,\n",
    "            sagemaker_session=self.sagemaker_session,\n",
    "        )\n",
    "\n",
    "        predictor = model.deploy(\n",
    "            initial_instance_count=1,\n",
    "            instance_type=\"ml.m5.large\",\n",
    "            endpoint_name=\"churn-prediction-endpoint\",\n",
    "        )\n",
    "        return predictor\n",
    "\n",
    "    def deploy_model_with_sagemaker_endpoint(self):\n",
    "        predictor = self.sagemaker_endpoint()\n",
    "        return predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0b0d75-c7f6-410d-b07a-b160d6e7287a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T09:43:00.129112Z",
     "iopub.status.busy": "2025-09-22T09:43:00.128433Z",
     "iopub.status.idle": "2025-09-22T09:43:00.133788Z",
     "shell.execute_reply": "2025-09-22T09:43:00.133008Z",
     "shell.execute_reply.started": "2025-09-22T09:43:00.129080Z"
    }
   },
   "outputs": [],
   "source": [
    "def entry_point():\n",
    "    raw_data_s3_uri = \"s3://sagemakerantdata/smallchurndataset/raw/\"\n",
    "    output_bucket = \"sagemakerantdata\"\n",
    "    output_prefix = \"smallchurndataset/artifacts\"\n",
    "    artifacts_path = \"sagemakerantdata/smallchurndataset/artifacts\"\n",
    "    pipeline_instance = ChurnPredictionPipeline(\n",
    "        raw_data_s3_uri=raw_data_s3_uri,\n",
    "        output_bucket=output_bucket,\n",
    "        output_prefix=output_prefix,\n",
    "        artifacts_path=artifacts_path\n",
    "    )\n",
    "    return pipeline_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db2f8bb-fcaa-4c06-bc68-95229659fd9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T09:45:21.882484Z",
     "iopub.status.busy": "2025-09-22T09:45:21.882196Z",
     "iopub.status.idle": "2025-09-22T09:45:22.554302Z",
     "shell.execute_reply": "2025-09-22T09:45:22.553552Z",
     "shell.execute_reply.started": "2025-09-22T09:45:21.882461Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_instance = entry_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9d98204-6e84-4e2b-938f-17698ce53d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T09:46:23.521187Z",
     "iopub.status.busy": "2025-09-22T09:46:23.520870Z",
     "iopub.status.idle": "2025-09-22T09:46:26.401838Z",
     "shell.execute_reply": "2025-09-22T09:46:26.400910Z",
     "shell.execute_reply.started": "2025-09-22T09:46:23.521162Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "/opt/conda/lib/python3.12/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "pipeline_instance.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9f60c4f-8bb4-45b4-a89e-c03c800d6b3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T09:54:56.797861Z",
     "iopub.status.busy": "2025-09-22T09:54:56.797554Z",
     "iopub.status.idle": "2025-09-22T09:54:56.963122Z",
     "shell.execute_reply": "2025-09-22T09:54:56.962419Z",
     "shell.execute_reply.started": "2025-09-22T09:54:56.797838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TuneLogisticRegression → Executing\n",
      "TuneRandomForest → Executing\n",
      "ChurnDataPrep → Succeeded\n"
     ]
    }
   ],
   "source": [
    "pipeline_instance.check_pipeline_status()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
